# -*- coding: utf-8 -*-
"""DL2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CAbCYsRQjuc1zWl0apXw31_Asb5zm4e4

dl class 18 - 20;

Customer churn prediction using ANN - 18 : https://chatgpt.com/c/68c2e38e-4f78-832e-9b3b-b605b19f86ac
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from google.colab import files
files.upload()  # Upload kaggle.json here

df = pd.read_csv('Customer-Churn.csv')
df

df.drop('customerID',axis='columns',inplace=True)

df.dtypes

df.TotalCharges.values

pd.to_numeric(df.TotalCharges)

pd.to_numeric(df.TotalCharges,errors='coerce').isnull()

df[pd.to_numeric(df.TotalCharges,errors='coerce').isnull()]

df.shape

df.iloc[488].TotalCharges

df[df.TotalCharges != ' '].shape

df1 = df[df.TotalCharges != ' ']
df1.shape

df1.dtypes

df1.TotalCharges = pd.to_numeric(df1.TotalCharges)

df1.TotalCharges.values

df1[df1.Churn=='No']

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
tenure_churn_no = df1[df1.Churn=='No'].tenure
tenure_churn_yes = df1[df1.Churn=='Yes'].tenure

plt.xlabel("tenure")
plt.ylabel("Number Of Customers")
plt.title("Customer Churn Prediction Visualiztion")

blood_sugar_men = [113, 85, 90, 150, 149, 88, 93, 115, 135, 80, 77, 82, 129]
blood_sugar_women = [67, 98, 89, 120, 133, 150, 84, 69, 89, 79, 120, 112, 100]

plt.hist([tenure_churn_yes, tenure_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])
plt.legend()

mc_churn_no = df1[df1.Churn=='No'].MonthlyCharges
mc_churn_yes = df1[df1.Churn=='Yes'].MonthlyCharges

plt.xlabel("Monthly Charges")
plt.ylabel("Number Of Customers")
plt.title("Customer Churn Prediction Visualiztion")

blood_sugar_men = [113, 85, 90, 150, 149, 88, 93, 115, 135, 80, 77, 82, 129]
blood_sugar_women = [67, 98, 89, 120, 133, 150, 84, 69, 89, 79, 120, 112, 100]

plt.hist([mc_churn_yes, mc_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])
plt.legend()

def print_unique_col_values(df):
       for column in df:
            if df[column].dtypes == 'object':
                print(f'{column}: {df[column].unique()}')

print_unique_col_values(df1)

df1.replace('No internet service','No',inplace=True)

print_unique_col_values(df1)

yes_no_columns = ['Partner','Dependents','PhoneService','MultipleLines','OnlineSecurity','OnlineBackup',
                  'DeviceProtection','TechSupport','StreamingTV','StreamingMovies','PaperlessBilling','Churn']
for col in yes_no_columns:
    df1[col].replace({'Yes': 1,'No': 0},inplace=True)



for col in df1:
    print(f'{col}: {df1[col].unique()}')

df1['gender'].replace({'Female':1,'Male':0},inplace=True)

df1.gender.unique()

df2 = pd.get_dummies(data=df1, columns=['InternetService','Contract','PaymentMethod'])
df2.columns

df2.sample(5)

df2.dtypes

cols_to_scale = ['tenure','MonthlyCharges','TotalCharges']

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df2[cols_to_scale] = scaler.fit_transform(df2[cols_to_scale])

for col in df2:
    print(f'{col}: {df2[col].unique()}')

X = df2.drop('Churn',axis='columns')
y = df2['Churn']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=5)

X_train.shape

X_test.shape

X_train[:10]

len(X_train.columns)

import tensorflow as tf
from tensorflow import keras


model = keras.Sequential([
    keras.layers.Dense(26, input_shape=(26,), activation='relu'),
    keras.layers.Dense(15, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

# opt = keras.optimizers.Adam(learning_rate=0.01)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10)

model.evaluate(X_test, y_test)

yp = model.predict(X_test)
yp[:5]

y_pred = []
for element in yp:
    if element > 0.5:
        y_pred.append(1)
    else:
        y_pred.append(0)

y_pred[:10]

y_test[:10]

from sklearn.metrics import confusion_matrix , classification_report

print(classification_report(y_test,y_pred))

import seaborn as sn
cm = tf.math.confusion_matrix(labels=y_test,predictions=y_pred)

plt.figure(figsize = (10,7))
sn.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')

y_test.shape

round((862+229)/(862+229+137+179),2)

round(862/(862+179),2)

round(229/(229+137),2)

round(862/(862+137),2)

round(229/(229+179),2)

# Take this dataset for bank customer churn prediction : https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling

# 1) Build a deep learning model to predict churn rate at bank.

# 2) Once model is built, print classification report and analyze precision, recall and f1-score



"""Precision, Recall, F1 score, True Positive - 19: https://chatgpt.com/c/68c2e6c6-3564-832f-a488-4bfc8b205203"""

from matplotlib import pyplot as plt
from sklearn.metrics import confusion_matrix , classification_report
import pandas as pd

from matplotlib import pyplot as plt
from sklearn.metrics import confusion_matrix , classification_report
import pandas as pd
import seaborn as sns

truth =      ["Dog","Not a dog","Dog","Dog",      "Dog", "Not a dog", "Not a dog", "Dog",       "Dog", "Not a dog"]
prediction = ["Dog","Dog",      "Dog","Not a dog","Dog", "Not a dog", "Dog",       "Not a dog", "Dog", "Dog"]

cm = confusion_matrix(truth,prediction)
print_confusion_matrix(cm,["Dog","Not a dog"])

print(classification_report(truth, prediction))

2*(0.57*0.67/(0.57+0.67))

2*(0.33*0.25/(0.33+0.25))

"""Dropout Regularization -> 20: https://chatgpt.com/c/68c2ea78-33c4-8326-a37b-9d7d9722fcb0"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
from google.colab import files
files.upload()  # Upload kaggle.json here

import warnings
warnings.filterwarnings('ignore')
df = pd.read_csv("sonar_dataset.csv", header=None)
df.sample(5)

df.shape

df.isna().sum()

df.columns

df[60].value_counts()

x = df.drop(60, axis='columns')
y = df[60]
y

y = pd.get_dummies(y, drop_first=True)
y.sample(5)

y.value_counts()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)



X_train.shape, X_test.shape

model = keras.Sequential([
    keras.layers.Dense(60, input_dim=60, activation='relu'),
    keras.layers.Dense(30, activation='relu'),
    keras.layers.Dense(15, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=8)

y_pred = model.predict(X_test).reshape(-1)
print(y_pred[:10])
# round the values to nearest integer in 0 to 1
y_pred = np.round(y_pred)
print(y_pred[:10])

model.evaluate(X_test, y_test)

from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(y_test, y_pred))



"""
# **Model with Dropout Layer**"""

modeld = keras.Sequential([
    keras.layers.Dense(60, input_dim=60, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(30, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(15, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(1, activation='sigmoid')
])

modeld.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

modeld.fit(X_train, y_train, epochs=20, batch_size=8)

modeld.evaluate(X_test, y_test)

y_pred = modeld.predict(X_test).reshape(-1)
print(y_pred[:10])

# round the values to nearest integer ie 0 or 1
y_pred = np.round(y_pred)
print(y_pred[:10])

from sklearn.metrics import confusion_matrix , classification_report

print(classification_report(y_test, y_pred))

# compare with before and after dropout
# precision    recall  f1-score   support

#        False       0.72      0.85      0.78        27
#         True       0.80      0.64      0.71        25

#     accuracy                           0.75        52
#    macro avg       0.76      0.75      0.75        52
# weighted avg       0.76      0.75      0.75        52